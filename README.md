
# Pretrained Models Evaluation (Keras/TensorFlow)

This project demonstrates how to **evaluate and test pre-trained models** (e.g., VGG16) on a custom dataset using **Google Colab** and **Keras/TensorFlow**.


## üìå Features
- Prepare training and testing data with `ImageDataGenerator`
- Evaluate a **pre-trained CNN** (default: VGG16) on your dataset
- Flexible paths for easy dataset replacement

---

### üîπ Data Preparation
- Uses `ImageDataGenerator(rescale=1./255)` to normalize pixel values between **0 and 1**.
- Loads images directly from directories (`train/` and `test/`) using `flow_from_directory`:
  - `target_size=(224, 224)` ‚Üí resizes images to match model input size.
  - `batch_size=32` ‚Üí processes 32 images at a time.
  - `class_mode='binary'` ‚Üí for binary classification tasks.

### üîπ Loading a Pre-trained Model
- The notebook imports **VGG16** from `tensorflow.keras.applications`.
- We load it with:
  ```python
  from tensorflow.keras.applications import VGG16
  base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))

### üîπ Model Architecture
The model is a custom **Convolutional Neural Network (CNN)** with:
- **3 Convolutional layers** (`Conv2D`) each followed by `MaxPooling2D`
- **Flatten layer** to convert 2D feature maps into 1D
- **Fully connected Dense layer** with dropout regularization
- **Output layer** with softmax activation for 2-class classification

| Layer (type)       | Output Shape       | Param #     |
|--------------------|-------------------|-------------|
| Conv2D (64)        | (None, 222, 222, 32) | 896       |
| MaxPooling2D       | (None, 111, 111, 32) | 0         |
| Conv2D (128)       | (None, 109, 109, 64) | 18,496    |
| MaxPooling2D       | (None, 54, 54, 64)   | 0         |
| Conv2D (128)       | (None, 52, 52, 128)  | 73,856    |
| MaxPooling2D       | (None, 26, 26, 128)  | 0         |
| Flatten            | (None, 86528)       | 0         |
| Dense (128)        | (None, 128)         | 11,075,712|
| Dropout (0.5)      | (None, 128)         | 0         |
| Dense (2)          | (None, 2)           | 258       |
| **Total params**   | **11,169,218**      | **42.61 MB** |

---

### üîπ Training Performance
We trained the model for **8 epochs** using:
- **Optimizer:** Adam
- **Loss:** categorical_crossentropy
- **Metrics:** accuracy

#### üìâ Loss over Epochs
![Loss over Epochs](images/loss_plot.png)

#### üìà Accuracy over Epochs
![Accuracy over Epochs](images/accuracy_plot.png)

**Observations:**
- Training and validation loss both decreased over time, indicating learning progress.
- Validation accuracy consistently outperformed training accuracy, suggesting good generalization.

## üöÄ Getting Started

### 1Ô∏è‚É£ Open in Google Colab
You can open the notebook directly in Colab:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR-USERNAME/pretrained-models-eval-keras/blob/main/Evaluating_and_Testing_Pretrained_Models.ipynb)

---

### 2Ô∏è‚É£ Dataset Preparation
1. Upload your `.zip` dataset to Google Drive.
2. Update the variables in the first cell:
```python
zip_path = '/content/drive/MyDrive/archive.zip'  # Path to your dataset zip file
extract_path = '/content/dataset'                # Extract location in Colab

pip install -r requirements.txt

